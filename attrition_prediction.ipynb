{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Employee Attrition Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the dataset, why we are performing predictive modelling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first -- we need to understand the data structure and choose relevant features for our predictive modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>Shift</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1313919</td>\n",
       "      <td>41</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200302</td>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Maternity</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1060315</td>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Maternity</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1272912</td>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Maternity</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1414939</td>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Maternity</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeID  Age Attrition     BusinessTravel  DailyRate  Department  \\\n",
       "0     1313919   41        No      Travel_Rarely       1102  Cardiology   \n",
       "1     1200302   49        No  Travel_Frequently        279   Maternity   \n",
       "2     1060315   37       Yes      Travel_Rarely       1373   Maternity   \n",
       "3     1272912   33        No  Travel_Frequently       1392   Maternity   \n",
       "4     1414939   27        No      Travel_Rarely        591   Maternity   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  ...  \\\n",
       "0                 1          2  Life Sciences              1  ...   \n",
       "1                 8          1  Life Sciences              1  ...   \n",
       "2                 2          2          Other              1  ...   \n",
       "3                 3          4  Life Sciences              1  ...   \n",
       "4                 2          1        Medical              1  ...   \n",
       "\n",
       "   RelationshipSatisfaction StandardHours  Shift  TotalWorkingYears  \\\n",
       "0                         1            80      0                  8   \n",
       "1                         4            80      1                 10   \n",
       "2                         2            80      0                  7   \n",
       "3                         3            80      0                  8   \n",
       "4                         4            80      1                  6   \n",
       "\n",
       "   TrainingTimesLastYear WorkLifeBalance  YearsAtCompany YearsInCurrentRole  \\\n",
       "0                      0               1               6                  4   \n",
       "1                      3               3              10                  7   \n",
       "2                      3               3               0                  0   \n",
       "3                      3               3               8                  7   \n",
       "4                      3               3               2                  2   \n",
       "\n",
       "   YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                        0                     5  \n",
       "1                        1                     7  \n",
       "2                        0                     0  \n",
       "3                        3                     0  \n",
       "4                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(r'watson_healthcare_modified.csv')\n",
    "\n",
    "# Display the first few rows of the data from our newly generated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predicting healthcare employee attrition, we need to outline a project plan:\n",
    "\n",
    "1. **Preprocess the Data**: Convert categorical variables to numerical, handle missing values if any, and select relevant features for predicting attrition.\n",
    "2. **Feature Selection**: Choose features that are likely to influence attrition based on domain knowledge and data exploration. For maximizing the F1 score, we'll focus on features that have a strong relationship with attrition.\n",
    "3. **Model Training**: Train a machine learning model using the selected features.\n",
    "4. **Prediction and Evaluation**: Predict attrition and evaluate the model using the F1 score.\n",
    "\n",
    "For initial feature selection, I chose the following features that would commonly relate to employee attrition:\n",
    "* Age\n",
    "* BusinessTravel\n",
    "* Department\n",
    "* DistanceFromHome\n",
    "* EducationField\n",
    "* JobRole\n",
    "* MaritalStatus\n",
    "* MonthlyIncome\n",
    "* OverTime\n",
    "* TotalWorkingYears\n",
    "* WorkLifeBalance\n",
    "* YearsAtCompany\n",
    "* YearsSinceLastPromotion\n",
    "* YearsInCurrentRole\n",
    "\n",
    "I will need to encode categorial features and use a model well-suited for mixed-type data. I will begin by using a Gradient Boosting Classifier. The features and model used will be reassessed following the output/analysis of our first trial's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 - Preprocessing, Feature Selection, and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in 'Attrition': 0\n",
      "Accuracy: 0.8958333333333334\n",
      "Precision: 0.7142857142857143\n",
      "Recall: 0.425531914893617\n",
      "F1 Score: 0.5333333333333333\n",
      "Confusion Matrix:\n",
      "[[281   8]\n",
      " [ 27  20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "# Preprocessing: Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus', 'OverTime']:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Convert Attrition to numerical\n",
    "df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Check for NaN values in the target variable and drop rows with NaN values\n",
    "print(f\"NaN values in 'Attrition': {df['Attrition'].isnull().sum()}\")\n",
    "df = df.dropna(subset=['Attrition'])\n",
    "\n",
    "# Selecting relevant features\n",
    "features = ['Age', 'BusinessTravel', 'Department', 'DistanceFromHome', 'EducationField', \n",
    "            'JobRole', 'MaritalStatus', 'MonthlyIncome', 'OverTime', 'TotalWorkingYears', \n",
    "            'WorkLifeBalance', 'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsInCurrentRole']\n",
    "\n",
    "X = df[features]\n",
    "y = df['Attrition']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using multiple metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Outputting all relevant performance information\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first results are in! Let's analyze them before diving into any tuning or additional modelling:\n",
    "\n",
    "#### Accuracy: 89.58%\n",
    "In our test data, we correctly predicted whether a healthcare employee would leave or stay at their job in <i>almost</i> 90% of cases. This sounds great at face value, but judging our model's performance based on accuracy alone would be misleading, especially when datasets are imbalanced (i.e. a certain demographic of healthcare employee is far more represented in the dataset). Let's dive into the other parameters for a better picture.\n",
    "\n",
    "#### Precision: 71.43%\n",
    "When the model predicted a healthcare employee would leave, it was correct about 71% of the time. If our healthcare company decided to increase spending to keep healthcare employees that were predicted to leave, false positives would prove very costly, which is a good reason to improve our performance in this area.\n",
    "\n",
    "#### Recall: 42.55%\n",
    "The model correctly identified 42.55% of all employees who actually left. In other words, our model missed more than half of the actual attrition cases in the test data. This suggests that the model is relatively conservative when predicting a healthcare employee will leave. If our healthcare company wants to identify as many true cases of attrition as possible for prevention measures, a low recall score like this would be a concern.\n",
    "\n",
    "#### F1 Score: 55.33%\n",
    "The F1 score is my personal favorite performance metric for machine learning models, as it illustrates the harmonic mean of precision and recall so that we may assess the balance between them. At 55.33%, I would say that this model has a moderate balance between the two, with solid room foor improvement. Our best case scenario is to improve recall without significantly sacrificing precision. \n",
    "\n",
    "#### Confusion Matrix: \n",
    "```\n",
    "[[281   8]\n",
    "[ 27  20]]\n",
    " ```\n",
    "\n",
    " This provides a numerical breakdown of the model's predictions:\n",
    "* <b>True Negatives (TN): 281</b> - The model correctly predicted that 281 employees would stay.\n",
    "* <b>False Positives (FP): 8</b> - The model incorrectly predicted that 8 employees would leave (but they stayed).\n",
    "* <b>False Negatives (FN): 27</b> - The model incorrectly predicted that 27 employees would stay (but they left).\n",
    "* <b>True Positives (TP): 20</b> - The model correctly predicted that 20 employees would leave.\n",
    "\n",
    "This further illustrates my point that our model is quite conservative when prediction attrition right now. This is evident due to the higher number of false negatives than false positives, which aligns with the lower recall score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2 - Hyperparameter Tuning (Mixed Automated and Manual Methods)\n",
    "This time, I performed a comprehensive grid search over a specified range of hyperparameters for the Gradient Boosting Classifier. This search had various values for the learning rate, maximum depth of the trees, and the number of estimators.\n",
    "\n",
    "The goal was to find the optimal combination of parameters that maximizes the F1 score, for which I utilized GridSearchCV. To ensure robustness, I evaluated across 4-fold cross-validation. I manually tweaked the test_size and random_state as well (somewhat arbitrarily) and assessed the performance number output.\n",
    "\n",
    "**WARNING**: Running this code is <i>EXTREMELY</i> CPU intensive!! This is only intended to be run on a powerful computer for finding the best tuning configuration! After we have these hyperparameters set, there's no need to perform the grid search each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 150}\n",
      "Accuracy: 0.9108910891089109\n",
      "Precision: 0.7272727272727273\n",
      "Recall: 0.5714285714285714\n",
      "F1 Score: 0.64\n",
      "Confusion Matrix:\n",
      "[[168   6]\n",
      " [ 12  16]]\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'watson_healthcare_modified.csv')\n",
    "\n",
    "# Encode categorical variables and prepare features and target\n",
    "label_encoders = {}\n",
    "categorical_columns = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus', 'OverTime']\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "features = ['Age', 'BusinessTravel', 'Department', 'DistanceFromHome', 'EducationField', \n",
    "            'JobRole', 'MaritalStatus', 'MonthlyIncome', 'OverTime', 'TotalWorkingYears', \n",
    "            'WorkLifeBalance', 'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsInCurrentRole']\n",
    "X = df[features]\n",
    "y = df['Attrition']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.12, random_state=40)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting Classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'max_depth': [3, 4, 5, 10],\n",
    "    'learning_rate': [0.1, 0.15, 0.05, 0.01, 0.001, 0.2, 0.25]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=40), param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_f1_score = grid_search.best_score_\n",
    "\n",
    "# Making predictions using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using multiple metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Outputting all relevant performance information\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Note: Replace 'path_to_your_data/watson_healthcare_modified.csv' with the actual path to your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best parameters found ({'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 150}), the model demonstrated clear improvement in its ability to predict employee attrition!\n",
    "Performance metrics were calculated, showing enhanced accuracy (91.09%), precision (72.73%), recall (57.14%), and F1 score (0.64), indicating a more balanced trade-off between precision and recall compared to previous attempts.\n",
    "The confusion matrix revealed that the model correctly predicted a significant majority of both positive (attrition) and negative (non-attrition) cases, with relatively few false positives and false negatives.\n",
    "\n",
    "I would be far more confident implementing this tuning of our model for a phase 1 predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Valuable Visualizations in Python\n",
    "Executive decision makers would be far more interested in learning what the most important factors (positive or negative) are for influencing employee attrition than they would be learning about the model's f1 score! Now it's time for me to turn our model's construction into meaningful, visual insights.\n",
    "\n",
    "I am going to calculate SHAP values (SHapley Additive exPlanations values) for the Gradient Boosting Classifier model and then plot them. This visualization allows us to interpret the contribution of each feature to our model's prediction for an individual instance.  In practice, we would likely undergo a few more trials of model/training data improvement before presenting a visualization like this (because this visualization is only as meaningful as your model is correct), but I am going to do it now for the sake of demonstrating how it can be done.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
